{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "**Tiền xử lí dữ liệu (data pre-processing)** là quá trình chuyển đổi (biến đổi) dữ liệu thô thành dữ liệu có định dạng mới giúp việc huấn luyện mô hình hiệu quả hơn.\n",
    "\n",
    "### Các bước tiền xử lí dữ liệu\n",
    "\n",
    " **1. Data Cleaning:** Dữ liệu có thể thừa các thông tin không cần thiết, thiếu dữ liệu. Chúng ta cần loại bỏ thông tin dư thừa và thêm đầy đủ các thông tin bị thiếu.\n",
    " \n",
    " **2. Data Transformation:** Dữ liệu sẽ được chuyển sang định dạng khác phù hợp hơn cho việc huấn luyện \n",
    " \n",
    " **3. Data Reduction:** Huấn luyện các mô hình học máy hay học sâu cần số lượng dữ liệu lớn, và sẽ tăng theo thời gian khi chúng ta huấn luyện các mô hình với các version khác nhau. Nên sử dụng kĩ thuật giảm dữ liệu nhằm tối đa hóa sức mạnh tính toán, giảm chi phí lưu trữ và phân tích dữ liệu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "**Tách từ (Tokenization)** là một kĩ thuật trong Xử lí ngôn ngữ tự nhiên (Natrual Language Processing), nhiệm vụ là chia các câu văn thành các từ riêng lẻ (words hoặc tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('brown')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('names')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu test\n",
    "example_text = \"\"\"Text preprocessing is the process of getting the raw text into \n",
    "                  a form which can be vectorized and subsequently consumed by machine learning \n",
    "                  algorithms for natural language processing (NLP) tasks such as text classification, \n",
    "                  topic modeling, name entity recognition\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tokenized words: ['Text', 'preprocessing', 'is', 'the', 'process', 'of', 'getting', 'the', 'raw', 'text', 'into', 'a', 'form', 'which', 'can', 'be', 'vectorized', 'and', 'subsequently', 'consumed', 'by', 'machine', 'learning', 'algorithms', 'for', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', 'such', 'as', 'text', 'classification', ',', 'topic', 'modeling', ',', 'name', 'entity', 'recognition']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tách từ và hiển thị các từ \n",
    "nltk_words = word_tokenize(example_text)\n",
    "display(f\"Tokenized words: {nltk_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning: Loại bỏ các nội dung không mong muốn \n",
    "\n",
    "* **Stop word:** Là những từ thường xuất hiện trong câu không bổ sung thêm ý nghĩa cho câu, nhưng lại xuất hiện rất phổ biến.\n",
    "* **Punctuation removal**: Loại bỏ dấu câu, dấu câu đôi khi không mang nhiều ý nghĩa trong việc mã hóa các đoạn văn. Nhưng nên sử dụng bước này sau khi tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation symbols: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['Text', 'preprocessing', 'is', 'the', 'process', 'of', 'getting', 'the', 'raw', 'text', 'into', 'a', 'form', 'which', 'can', 'be', 'vectorized', 'and', 'subsequently', 'consumed', 'by', 'machine', 'learning', 'algorithms', 'for', 'natural', 'language', 'processing', 'NLP', 'tasks', 'such', 'as', 'text', 'classification', 'topic', 'modeling', 'name', 'entity', 'recognition']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(f\"Punctuation symbols: {string.punctuation}\")\n",
    "\n",
    "nltk_words = list(t for t in nltk_words if t not in string.punctuation)\n",
    "print(nltk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "ntlk stop words count: 179\n"
     ]
    }
   ],
   "source": [
    "# Chúng ta hãy kiểm tra xem trong Tiếng Anh có những Stop word nào\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "print(f\"ntlk stop words count: {len(stopwords.words('english'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'preprocessing', 'is', 'the', 'process', 'of', 'getting', 'the', 'raw', 'text', 'into', 'a', 'form', 'which', 'can', 'be', 'vectorized', 'and', 'subsequently', 'consumed', 'by', 'machine', 'learning', 'algorithms', 'for', 'natural', 'language', 'processing', 'NLP', 'tasks', 'such', 'as', 'text', 'classification', 'topic', 'modeling', 'name', 'entity', 'recognition']\n",
      "----------------------------------------------------------------------------------\n",
      "['Text', 'preprocessing', 'process', 'getting', 'raw', 'text', 'form', 'vectorized', 'subsequently', 'consumed', 'machine', 'learning', 'algorithms', 'natural', 'language', 'processing', 'NLP', 'tasks', 'text', 'classification', 'topic', 'modeling', 'name', 'entity', 'recognition']\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ các stop word trên các câu \n",
    "stop_words = stopwords.words('english')\n",
    "filtered_sentence = [w for w in nltk_words if not w in stop_words] \n",
    "filtered_sentence = list(filter(lambda x: x not in stop_words, nltk_words))\n",
    "print(nltk_words)\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "* **Normalization:** Là quá trình chuyển đổi thông tin không phải là văn bản thành văn bản tương đương\n",
    "* **Ví dụ**\n",
    "    + Chuyển đổi ngày tháng (số) thành chữ \n",
    "    + Chuyển đổi số sang chữ \n",
    "    + Kí hiệu tiền tệ, phần trăm sang chữ \n",
    "    + Từ viết tắt -> Thành từ không viết tắt \n",
    "    + Lỗi chính tả (teencode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized text: On the thirteenth of February two thousand and seven , Theresa May announced on M T V news that the rate of childhood obesity had risen from seven point three to nine point six % in just three years , costing the North Atlantic Treaty Organization twenty million pounds\n"
     ]
    }
   ],
   "source": [
    "from normalise import normalise\n",
    "\n",
    "text = \"\"\"\n",
    "On the 13 Feb. 2007, Theresa May announced on MTV news that the rate of childhod obesity had \n",
    "risen from 7.3-9.6% in just 3 years , costing the N.A.T.O £20m\n",
    "\"\"\"\n",
    "\n",
    "user_abbr = {\n",
    "    \"N.A.T.O\": \"North Atlantic Treaty Organization\"\n",
    "}\n",
    "\n",
    "normalized_tokens = normalise(word_tokenize(text), user_abbrevs=user_abbr, verbose=False)\n",
    "print(f\"Normalized text: {' '.join(normalized_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "\n",
    "Trong quá trình xử lí ngôn ngữ tự nhiên, việc so sánh hai từ (token) với nhau là điều cần thiết. Không sử dụng toán tử **==** là có thể kiếm tra đầy đủ. Ví dụ các từ “walks“, “walking“, “walked” đều là các biến thể của từ “walk” và đều mang ý nghĩa là “đi bộ”. Có hai kỹ thuật chính để giải quyết vấn đề này.\n",
    "\n",
    "* **Stemming:** là kỹ thuật biến đổi 1 từ về dạng gốc (gọi là stem hoặc root form), bằng cách loại bỏ các kí tự ở cuối mà nó nghĩ đó là biến thể của từ gốc (vd -ed, -ing, -s...)\n",
    "* **Lemmatization:** là kĩ thuật bằng cách so sánh trong 1 bộ dictionary nào đó, và chắc chắn nó sẽ cho kết quả chính xác cao hơn, nhưng nhược điểm là tốc độ vì nó phải kiểm tra hết trong dữ liệu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.vectorize object at 0x7f183d61bba8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Stemed text: On the thirteenth of februari two thousand and seven , theresa may announc on M T V news that the rate of childhood obes had risen from seven point three to nine point six % in just three year , cost the north atlant treati organ twenti million pound'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "text = ' '.join(normalized_tokens)\n",
    "tokens = word_tokenize(text)\n",
    "porter=PorterStemmer()\n",
    "\n",
    "stem_words = np.vectorize(porter.stem) # vector\n",
    "stemed_text = ' '.join(stem_words(tokens))\n",
    "print(f\"Stemed text: {stemed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk lemmatized text: On the thirteenth of February two thousand and seven , Theresa May announced on M T V news that the rate of childhood obesity had risen from seven point three to nine point six % in just three year , costing the North Atlantic Treaty Organization twenty million pound\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\n",
    "lemmatized_text = ' '.join(lemmatize_words(tokens))\n",
    "print(f\"nltk lemmatized text: {lemmatized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Một bài tập nho nhỏ, các bạn sẽ xây dựng một class tự động hóa xử lí dữ liệu theo các bước như sau:\n",
    "    1. Text normalization\n",
    "    2. Punctuation removal\n",
    "    3. Stop words removal\n",
    "    4. Lemmatization\n",
    "    5. Tokenization\n",
    "    \n",
    "#### Dữ liệu cần xử lí: data/30569_38997_compressed_bbc-text.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nguồn tham khảo\n",
    "\n",
    "* **Data Preprocessing in Data Mining:** https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
